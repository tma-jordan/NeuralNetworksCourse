# -*- coding: utf-8 -*-
"""framework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ld5nFgC2KzKoIDGNGLkDL7oXs8orXOu
"""

import os
import numpy as np
import matplotlib.pyplot as plt
plt.switch_backend("agg")


class ANN(object):
    def __init__(
        self,
        model=None,
        error_fun=None,
        printer=None,
    ):
        self.layers = model
        self.error_fun = error_fun
        self.error_history = []                           #List to capture error history
        self.n_iter_train = int(8e5)                      #Number of iterations in training set
        self.n_iter_evaluate = int(2e5)                   #Number of iterations in evaluation set
        self.viz_interval = int(1e5)                      #Number of iterations at which the error visualisation updates each time
        self.reporting_bin_size = int(1e3)                #Run this number of times and average to spot trends
        self.report_min = -3                              #Minimum amount the report focuses on - so the graph focuses on the right area
        self.report_max = 0                               #Maximum amount the report focuses on - so the graph focuses on the right area
        self.printer = printer

        self.reports_path = "reports"                     #File settings to save visualisation
        self.report_name = "performance_history.png"      #File settings to save visualisation
        # Ensure that subdirectories exist.
        try:
            os.mkdir("reports")                           #Try using OS package to make a reports directory
        except Exception:
            pass                                          #If the reports directory isn't created, move on

    def train(self, training_set):
        #For each iteration
        for i_iter in range(self.n_iter_train):
            #The input is taken as the next in line from the training set and turned into a 1-dimensional vector
            x = next(training_set()).ravel()
            #The model uses forward propagation to estimate the output y given the input x
            y = self.forward_pass(x)
            #Calculate model error - the difference between the estimate y and the actual data, which is the same as the input - changed??
            error = self.error_fun.calc(y)
            #Calculate the slope of error function or loss curve
            error_d = self.error_fun.calc_d(y)
            #Add error record to error_history list
            self.error_history.append(error)
            #Run backpropagation to improve the model accuracy, inputting the the slope of the error function so we
            #move down the gradient and to a minimum
            self.backward_pass(error_d)

            #When we reach an iteration that corresponds with a reporting individual (i.e. remainder is 0)
            if (i_iter + 1) % self.viz_interval == 0:
                #...run report() function
                self.report()
                self.printer.render(self, x, f"train_{i_iter + 1:08d}")


    def evaluate(self, evaluation_set):
        for i_iter in range(self.n_iter_evaluate):
            x = next(evaluation_set()).ravel()
            #Run forward propagation - let the method know we are evaluating so the model does not run dropout
            y = self.forward_pass(x, evaluating=True)
            #Calculate model error - how does x and y work?
            error = self.error_fun.calc(y)
            #Add error record to error_history list
            self.error_history.append(error)

            #When we reach an iteration that corresponds with a reporting individual (i.e. remainder is 0)
            if (i_iter + 1) % self.viz_interval == 0:
                #...run report() function
                self.report()
                self.printer.render(self, x, f"eval_{i_iter + 1:08d}")

    #Forward propagation function
    def forward_pass(
        self,
        #Inputs vector
        x,
        #Bool to tell us whether we are testing or evaluating the model, some layers may act differently in training
        evaluating = False,
        #Which layer to start at - helpful for visualisation
        i_start_layer = None,
        #Which layer to stop before - helpful for visualisation
        i_stop_layer = None
    ):

        #Set start and stop layers if no information is provided
        if i_start_layer is None:
            i_start_layer = 0
        if i_stop_layer is None:
            i_stop_layer = len(self.layers)
        #Correct for invalid entries in start and stop layer
        if i_start_layer >= i_stop_layer:
            return x
        #We probably also want to do the same if the stop layer is too high or start layer negative ??

        #Reset all the layers to get them ready for the new iteration
        for layer in self.layers:
            layer.reset()

        # Convert the inputs into a 2D array of the right shape and increment the inputs of the start layer
        self.layers[i_start_layer].x += x.ravel()[np.newaxis, :]

        #Run through each layer as a loop. y is made the output for each layer, which is fed into the next layer.
        #We also pass forward whether the model is training or evaluating so we can enable (disable) dropout for training (evaluation)
        for layer in self.layers[i_start_layer: i_stop_layer]:
            layer.forward_pass(evaluating=evaluating)

        #The final output vector is returned once all layers are worked through. np.ravel() turns this into a one dimensional array
        return layer.y.ravel()

    #backpropagation method
    def backward_pass(self, de_dy):
        #For each  i, where i is an object belonging to the class layer - ?? changed, can see we pass deivatie through to minimise but a bit unclear
        #And where we work backwards from the last layer to the first layer
        self.layers[-1].de_dy += de_dy
        for layer in self.layers[::-1]:
            layer.backward_pass()

    #Error reporting and visualisation
    def report(self):
        n_bins = int(len(self.error_history) // self.reporting_bin_size)
        smoothed_history = []
        for i_bin in range(n_bins):
            smoothed_history.append(np.mean(self.error_history[
                i_bin * self.reporting_bin_size:
                (i_bin + 1) * self.reporting_bin_size
            ]))
        error_history = np.log10(np.array(smoothed_history) + 1e-10)
        ymin = np.minimum(self.report_min, np.min(error_history))
        ymax = np.maximum(self.report_max, np.max(error_history))
        fig = plt.figure()
        ax = plt.gca()
        ax.plot(error_history)
        ax.set_xlabel(f"x{self.reporting_bin_size} iterations")
        ax.set_ylabel("log error")
        ax.set_ylim(ymin, ymax)
        ax.grid()
        fig.savefig(os.path.join(self.reports_path, self.report_name))
        plt.close()
